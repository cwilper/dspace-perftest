#!/usr/bin/env bash

script_dir=$(cd $(dirname $0); pwd)
. $script_dir/.functions.sh

main() {
  if [[ $1 =~ / ]]; then
    _output_dir=$1
    require_file $(outfile samples.jtl)
  else
    local version="$1"
    local tests_dir=$(cd $script_dir/../tests; pwd)
    [[ $version ]] || die "Must specify directory or test"
    _output_dir="$tests_dir/$version/output"
    require_file $(outfile samples.jtl)
  fi
  # source config used with run
  . $(outfile plan.cfg)
  [[ $report_name ]] || die "Please define report_name in plan.cfg first"
  create_csvs
  create_pngs
  create_md
}

outfile() {
  echo "$_output_dir/$1"
}

create_csvs() {
  bold "Creating csv files from jtl"
  local jtl
  local csv

  # Extract page-level statistics
  jtl=$(outfile samples.jtl)
  csv=$(outfile pages.csv)
  require_file "$jtl"
  report "-> $csv"
  grep -E "^timeStamp|Number of samples" "$jtl" > "$csv"

  # Extract top url-level statistics
  csv=$(outfile requests.csv)
  report "-> $csv"
  grep -v "Number of samples" "$jtl" > "$csv"

  # Extract Home statistics
  csv=$(outfile home.csv)
  report "-> $csv"
  sed -e 's|,[^,-]*-A-Home|,Home|g' "$jtl" | grep -E '^timeStamp|,Home,' > "$csv"

  # Relabel cpu metrics
  jtl=$(outfile cpu.jtl)
  csv=$(outfile cpu.csv)
  if [[ -f $jtl ]]; then
    report "-> $csv"
    sed -e "s|,$host CPU,|,system CPU %,|g" $jtl > $csv
    if [[ $node_pid ]]; then
      sed -i '' "s|,$host CPU pid=$node_pid,|,node CPU %,|g" $csv
    fi
    if [[ $solr_pid ]]; then
      sed -i '' "s|,$host CPU pid=$solr_pid,|,solr CPU %,|g" $csv
    fi
    if [[ $tomcat_pid ]]; then
      sed -i '' "s|,$host CPU pid=$tomcat_pid,|,tomcat CPU %,|g" $csv
    fi
  fi

  # Relabel memory metrics
  jtl=$(outfile database.jtl)
  csv=$(outfile database.csv)
  if [[ -f $jtl ]]; then
    report "-> $csv"
    sed -e "s|,$host EXEC.*state=\'\(.*\)\',|,\1 connections,|g" $jtl > $csv
  fi

  # Relabel database metrics
  jtl=$(outfile memory.jtl)
  csv=$(outfile memory.csv)
  if [[ -f $jtl ]]; then
    report "-> $csv"
    sed -e "s|,$host Memory,|,system memory used %,|g" $jtl > $csv
    if [[ $node_pid ]]; then
      sed -i '' "s|,$host Memory pid=$node_pid,|,node bytes used,|g" $csv
    fi
    if [[ $solr_pid ]]; then
      sed -i '' "s|,$host EXEC .*heapbytes:$solr_pid,|,solr heap bytes used,|g" $csv
    fi
    if [[ $tomcat_pid ]]; then
      sed -i '' "s|,$host EXEC .*heapbytes:$tomcat_pid,|,tomcat heap bytes used,|g" $csv
    fi
  fi

  jtl=$(outfile hits.jtl)
  csv=$(outfile hits.csv)
  if [[ -f $jtl ]]; then
    report "-> $csv"
    cp $jtl $csv
  fi
}

create_pngs() {
  bold "Creating png files from csv"

  # First, determine path of JMeterPluginsCMD.sh and make sure it exists
  local jmeter_path=$(which jmeter)
  if [[ -e $jmeter_path ]]; then
    jmeter_path=$(readlink $jmeter_path)
  fi
  _cmd="$(cd $(dirname $jmeter_path); pwd)/JMeterPluginsCMD.sh"
  require_file "$_cmd"

  # Generate png for ReponseTimesOverTime and PerfMon outputs, as available
  generate_png ResponseTimesOverTime home home
  generate_png ThreadsStateOverTime hits threads
  generate_png ResponseCodesPerSecond hits codes
  for name in cpu database memory; do
    [[ -f $(outfile $name.csv) ]] && generate_png PerfMon $name $name
  done
}

generate_png() {
  local plugin_type=$1
  local csv=$(outfile $2.csv)
  local png=$(outfile $3.png)
  local logfile=$(mktemp)
  require_file "$csv"
  report "-> $png"
  if [[ -f $png ]]; then
    warn "PNG already exists; not regenerating"
    return
  fi
  "$_cmd" --plugin-type $plugin_type --line-weight 2 --width 1366 --height 768 --input-jtl "$csv" --granulation 10000 --generate-png "$png" > $logfile 2>&1 || local failed=true
  if [[ $failed ]]; then
    cat "$logfile"
    rm "$logfile"
    die "Failed to generate png"
  else
    rm "$logfile"
  fi
}

create_md() {
  bold "Creating README.md"
  md=$(outfile README.md)
  report "-> $md"

  # Compute and add averages
  jtl=$(outfile samples.jtl)

  declare -A session_names
  declare -A page_names
  declare -A total_bytes_by_page_name
  declare -A total_bytes_by_session_name
  declare -A total_ms_by_page_name
  declare -A total_ms_by_session_name
  declare -A num_samples_by_page_name
  declare -A num_samples_by_session_name
  declare -A url_by_name

  local n=0
  local ts1
  while read line; do
    if [[ $n > 0 ]]; then
      IFS="," read -ra arr <<< "$line"
      local page_name=${arr[2]}
      local ts=${arr[0]}
      if [[ $n = 1 ]]; then
        ts1=$ts
      fi
      local elapsed=$((ts-ts1))
      if [[ $elapsed -lt $((1000 * 60 * 12)) ]]; then
        IFS="-" read -ra arr2 <<< "$page_name"
        local session_name=${arr2[0]}

        local url_index=13
        local byte_index=$((url_index - 4))
        if [[ ${arr[4]} =~ "Number of samples" ]]; then
          page_names[$page_name]=$page_name
          session_names[$session_name]=$session_name
          byte_index=$((byte_index + 1))
        else
          url_by_name[$page_name]=${arr[$url_index]}
        fi

        local bytes=${arr[$byte_index]}
        local total_bytes=${total_bytes_by_page_name[$page_name]}
        total_bytes=$((total_bytes+bytes))
        total_bytes_by_page_name[$page_name]=$total_bytes

        total_bytes=${total_bytes_by_session_name[$session_name]}
        total_bytes=$((total_bytes+bytes))
        total_bytes_by_session_name[$session_name]=$total_bytes

        local ms=${arr[1]}
        local total_ms=${total_ms_by_page_name[$page_name]}
        total_ms=$((total_ms+ms))
        total_ms_by_page_name[$page_name]=$total_ms

        total_ms=${total_ms_by_session_name[$session_name]}
        total_ms=$((total_ms+ms))
        total_ms_by_session_name[$session_name]=$total_ms

        local num_samples=${num_samples_by_page_name[$page_name]}
        num_samples=$((num_samples+1))
        num_samples_by_page_name[$page_name]=$num_samples

        num_samples=${num_samples_by_session_name[$session_name]}
        num_samples=$((num_samples+1))
        num_samples_by_session_name[$session_name]=$num_samples
      fi
    fi
    n=$((n+1))
  done < <(cat $jtl)

  cat > $md << EOF
# $report_name

During this test, the number of concurrent threads started at one, and was doubled after 12 minutes,
doubling every 2 minutes after that, peaking at 32 threads, then ramped down to a single thread for
a couple minutes to end the test.

![](threads.png)

For detailed information about the user activities modeled in this test,
see [Sessions, transactions, and requests](../../doc/sessions).

## Single-threaded results

During the first phase of the test, a single thread was used to make all requests,
so the server only had to handle one request at a time. This put an artificially low load on the server,
while providing an average "best case" roundtrip time number per request.

**IMPORTANT: single-threaded != single-user**

The seconds/session numbers reported below were computed by adding all delays for loading
each resource for each page. A real browser would spend less time waiting for all resources
to load due to parallel loading of resources.  So, while helpful in understanding the overall
performance picture, **the seconds/session numbers do not accurately reflect expected page wait times**.

Session | Pages/session | Bytes/session | Seconds/session | Transaction details
-|-|-|-|-
EOF

  for session_name in $(printf '%s\n' ${!session_names[@]} | sort); do
    local num_pages=0
    for page_name in $(printf '%s\n' ${!page_names[@]}); do
      if [[ $page_name =~ ^$session_name- ]]; then
        num_pages=$((num_pages+1))
      fi
    done

    local total_avg_ms_for_session=0
    local total_avg_bytes_for_session=0

    for page_name in $(printf '%s\n' ${!page_names[@]} | sort); do
      if [[ $page_name =~ ^$session_name- ]]; then
        local names_for_page=()
        for name in "${!url_by_name[@]}"; do
          if [[ $name =~ ^$page_name ]]; then
            local url=${url_by_name[$name]}
            local u1=${url/*\/\//}
            IFS=/ read -ra u2<<<"$u1"
            url=${u1/${u2[0]}/}
            if [[ $url ]]; then
              names_for_page+=($name)
            fi
          fi
        done

        local num_samples=${num_samples_by_page_name[$page_name]}
        local avg_ms=$((${total_ms_by_page_name[$page_name]} / $num_samples))
        local avg_bytes=$((${total_bytes_by_page_name[$page_name]} / $num_samples))

        total_avg_ms_for_session=$((total_avg_ms_for_session + avg_ms))
        total_avg_bytes_for_session=$((total_avg_bytes_for_session + avg_bytes))
      fi
    done

    local sec=$(bc<<<"scale=1;${total_avg_ms_for_session}/1000")
    local size=$(pretty_bytes $total_avg_bytes_for_session)
    echo "$session_name | $num_pages | $size | $sec | [See details](details-${session_name,,})" >> $md
  done

    cat >> $md << EOF

## Test results over time

The following graphs show how certain measurements changed over the lifetime of the test.

### Homepage load time

This shows how the average time to load all home page resources, back-to-back, changed over the life of the test,
depending on how many threads were active.

![](home.png)

### Responses/sec by status code

![](codes.png)

### CPU use

![](cpu.png)

### Database connections

![](database.png)

### Memory use

![](memory.png)

EOF

  for session_name in $(printf '%s\n' ${!session_names[@]} | sort); do
    md=$(outfile details-${session_name,,}.md)
    report "-> $md"
    cat > $md << EOF
# "$session_name" session details

The following averages are computed over a number of samples taken for each request during
the initial single-threaded phase of the test.

For detailed information about the user activities associated with each "transaction" below,
see [Sessions, transactions, and requests](../../doc/sessions).

Transaction | Requests | Bytes/transaction | Seconds/transaction | Request details
-|-|-|-|-
EOF

    for page_name in $(printf '%s\n' ${!page_names[@]} | sort); do
      if [[ $page_name =~ ^$session_name- ]]; then
        local names_for_page=()
        for name in "${!url_by_name[@]}"; do
          if [[ $name =~ ^$page_name ]]; then
            local url=${url_by_name[$name]}
            local u1=${url/*\/\//}
            IFS=/ read -ra u2<<<"$u1"
            url=${u1/${u2[0]}/}
            if [[ $url ]]; then
              names_for_page+=($name)
            fi
          fi
        done

        local num_samples=${num_samples_by_page_name[$page_name]}
        local avg_ms=$((${total_ms_by_page_name[$page_name]} / $num_samples))
        local avg_bytes=$((${total_bytes_by_page_name[$page_name]} / $num_samples))
        cat >> $md << EOF
$page_name | ${#names_for_page[@]} | $(pretty_bytes $avg_bytes) | $(bc<<<"scale=2;${avg_ms}/1000") | [See below](#${page_name,,}-requests)
EOF
      fi
    done

    cat >> $md << EOF

## Request details

The following tables show average number of bytes and seconds per request.

Numbers **in bold** below are higher than most (>=1MB or >=1 second), and may warrant further investigation.

EOF

    for page_name in $(printf '%s\n' ${!page_names[@]} | sort); do
      if [[ $page_name =~ ^$session_name- ]]; then
        local names_for_page=()
        for name in "${!url_by_name[@]}"; do
          if [[ $name =~ ^$page_name ]]; then
            local url=${url_by_name[$name]}
            local u1=${url/*\/\//}
            IFS=/ read -ra u2<<<"$u1"
            url=${u1/${u2[0]}/}
            if [[ $url ]]; then
              names_for_page+=($name)
            fi
          fi
        done

        cat >> $md << EOF

### $page_name requests

Samples | Bytes/request | Seconds/request | Path
-|-|-|-
EOF

        while read name; do
          local url=${url_by_name[$name]}
          local u1=${url/*\/\//}
          IFS=/ read -ra u2<<<"$u1"
          url=${u1/${u2[0]}/}
          if [[ $url ]]; then
            num_samples=${num_samples_by_page_name[$name]}
            avg_ms=$((${total_ms_by_page_name[$name]} / $num_samples))
            avg_bytes=$((${total_bytes_by_page_name[$name]} / $num_samples))
            local sec=$(bc<<<"scale=3;${avg_ms}/1000")
            if [[ $avg_ms -ge 1000 ]]; then
              sec="**$sec**"
            fi
            local size=$(pretty_bytes $avg_bytes)
            if [[ $avg_bytes -ge 1000000 ]]; then
              size="**$size**"
            fi
            echo "$num_samples | $size | $sec | $url |" >> $md
          fi
        done < <(for name in "${!url_by_name[@]}"; do
                   if [[ $name =~ ^$page_name ]]; then
                     echo "$name"
                   fi
                 done | sort)
      fi
    done
  done
}

main "$@"
